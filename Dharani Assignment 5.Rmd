---
title: "FML Assigment-5"
author: "Dharani"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
#Using a library function, first we are loading the necessary packages.
```{r include=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendograms
library(sparcl)     #install.packages('sparcl') to create colour Dendograms
library(GGally)
library(dplyr)
```

#Data Pre-processing

#Counting the amount of missing values and then adding or deleting them.
```{r}
Cereals_Data1 <- read.csv("C:/Users/DHARANI/Downloads/Cereals.csv")
Cereals1<-read.csv("C:/Users/DHARANI/Downloads/Cereals.csv")
str(Cereals_Data1)
sum(is.na(Cereals_Data1))
```

#To eliminate any missing values from the data.
```{r}
Cereals_Data1 <- na.omit(Cereals_Data1)
Cereals1<-na.omit(Cereals1)
sum(is.na(Cereals_Data1))
```

#In order to see the clusters later, translate the cereal names into row names.
```{r}
rownames(Cereals_Data1) <- Cereals_Data1$name
rownames(Cereals1) <- Cereals1$name
```

#The name column should be removed as it is no longer helpful.
```{r}
Cereals_Data1$name = NULL
Cereals1$name = NULL
```

#Since factors with greater ranges would considerably impact the distance, the data must be corrected before measuring any distance.
```{r}
Cereals_Data1 <- scale(Cereals_Data1[,3:15])
```

#We will use Euclidean distance to do hierarchical clustering on the data.
```{r}
# Dissimilarity matrix
d <- dist(Cereals_Data1, method = "euclidean")
# Hierarchical clustering using Complete Linkage
HC_comp <- hclust(d, method = "complete" )
# Plot the obtained dendrogram
plot(HC_comp, cex = 0.6, hang = -1)
```

#the agglomerative coefficients of each technique, in addition to clustering based on average, complete, single
```{r}
library(cluster)
HC_single1 <- agnes(Cereals_Data1, method = "single")
pltree(HC_single1, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

```{r}
HC_avg <- agnes(Cereals_Data1, method = "average")
pltree(HC_avg, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

#We will calculate the agnes coefficient for each approach.
```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
# function to compute coefficient
ac <- function(x) {
  agnes(Cereals_Data1, method = x)$ac
}
map_dbl(m, ac) 
```
#Ward is the best linking method, with an agglomerative coefficient of 0.9046042.

#Using the wards approach to visualize the dendogram:

```{r}
HC_Wards <- agnes(Cereals_Data1, method = "ward")
pltree(HC_Wards, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

#Cut the dendogram with cutree() to find sub-groups (i.e. clusters):

```{r}
#Create the distance matrix
d <- dist(Cereals_Data1, method = "euclidean")
# Ward's method for Hierarchical clustering
HC_Ward_clust <- hclust(d, method = "ward.D2" )
plot(HC_Ward_clust, cex=0.6 )
rect.hclust(HC_Ward_clust,k=6,border = 1:6)
```

#Let's examine how many data records have been categorized and allocated to clusters:
```{r}
# Cut tree into 6 groups
sub_grup <- cutree(HC_Ward_clust, k = 6)
# Number of members in each cluster
table(sub_grup)
```

#Correlation matrix:
```{r}
#install.packages("GGally")
Cereals1 %>% 
  select(calories, protein, fat, sodium, fiber, carbo, sugars, potass,vitamins,rating) %>% 
  ggcorr(palette = "RdBu", label = TRUE, label_round =  2)
```

#With the use of the correlation matrix, we may ascertain if there is a strong or weak link between the variables. Our ability to compute descriptive statistics will increase. 

#P-values for hierarchical clustering based on multiscale bootstrap resampling are generated by the pvclust() function of the pvclust package. Large p values will be assigned to clusters that the data strongly support. Suzuki provides interpretive advice. Recall that columns are grouped in pvclust instead of rows. Prior to utilizing your data, make sure it has been transposed.

```{r results="hide"}
# Ward Hierarchical Clustering with Bootstrapped p values
#install.packages("pvclust")
library(pvclust)
fit.pv <- pvclust(Cereals_Data1, method.hclust="ward.D2",
               method.dist="euclidean")
```

```{r}
plot(fit.pv) # dendogram with p values
# add rectangles around groups highly supported by the data
pvrect(fit.pv, alpha=.95)
```

#The mean value of each cluster's Jaccard coefficient over all bootstrap rounds represents the cluster stability of each cluster in the original clustering. If a cluster's stability score is less than 0.6, it should be regarded as unstable. There isn't much agreement on which points belong together in the cluster, but it can detect a pattern in the data with values between 0.6 and 0.75. Clusters with stability scores greater than 0.85 are deemed highly stable.

1. The ideal approach is to maximize the cluster wise Bootstrapping Jaccard 
2. Dissolved clusters ought to be limited in number.
3. More recovered clusters should be added while remaining as close together as feasible. 

#Running clusterboot()
```{r results="hide"}
library(fpc)
library(cluster)
Kbest_p<-6
cboot_hclust <- clusterboot(Cereals_Data1,clustermethod=hclustCBI,method="ward.D2", k=Kbest_p)
```

```{r}
summary(cboot_hclust$result)
groups<-cboot_hclust$result$partition
head(data.frame(groups))
#The vector of cluster stabilities
cboot_hclust$bootmean
#The number of times each cluster was disintegrated. Clusterboot() executes 100 bootstrap by default. 
#iterations.
cboot_hclust$bootbrd
```

#The findings suggest that clusters 1 and 3 are quite stable. Despite the fact that Clusters 4 and 5 are identifying a pattern, there is disagreement on which points should be grouped together. Right now, clusters 2 and 5 are unstable.

#Extracting the clusters found by hclust()
```{r}
groups <- cutree(HC_Ward_clust, k = 6)
print_clusters <- function(labels, k) {
for(i in 1:k) {
print(paste("cluster", i))
print(Cereals1[labels==i,c("mfr","calories","protein","fat","sodium","fiber","carbo","sugars","potass",
                "vitamins","rating")])
}
}
print_clusters(groups, 6)
```

Notes:

Since there is no mention of an adequate metric or scale to create a healthy diet, my decision to select clusters rich in nutritional elements and based on statistical values to make a healthy diet is entirely subjective.

to determine whether or not normalization was required. No, in my opinion. Normalization reduces the data's magnitude, which makes analysis and decision-making more difficult.

The clusters have a rich, sufficient, and nutrient-poor cereal diet. The data was divided into six groupings, and these clusters will be examined in the context of all the variables and factors.

Although there are some nutritionally sound recommendations for establishing a balanced diet in Cluster 1, the selection is somewhat small. Clusters 2 and 3 have low ratings and high fat and sugar contents, thus they are not advised for a healthy supper.

Clusters 4 and 5 have well-balanced nutritional values and receive great marks from customers. Thus, if elementary public schools choose to use this in their cafeterias, Clusters 4 and 5 should be their best bets.